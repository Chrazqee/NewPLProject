import hydra
import pytorch_lightning as pl
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning.callbacks import LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.strategies import DDPStrategy

from modules.module_fetch import fetch_data_module, fetch_net_module


# config_path: æŒ‡å®š yaml æ–‡ä»¶åœ¨å“ªä¸ªç›®å½•ä¸‹
@hydra.main(config_path="conf", config_name="config", version_base="1.2")
def main(config: DictConfig):
    # å°† config æ‰“å°å‡ºæ¥çœ‹çœ‹
    print(OmegaConf.to_yaml(config))

    # å®šä¹‰è®­ç»ƒç­–ç•¥
    distributed_backend = config.training.hardware.dist_backend
    gpus = config.training.hardware.gpus
    assert distributed_backend in ('nccl', 'gloo'), f'{distributed_backend=}'
    strategy = DDPStrategy(process_group_backend=distributed_backend,
                           find_unused_parameters=False,
                           gradient_as_bucket_view=True) if len(gpus) > 1 else None

    # ---------------------
    # Data
    # ---------------------
    dataset_config = config.dataset_config
    assert dataset_config is not None
    dataset_name = config.dataset_name
    assert dataset_name is not None
    data_module = fetch_data_module(dataset_name=dataset_name,
                                    dataset_config=dataset_config)

    # ---------------------
    # Logging and Checkpoints
    # ---------------------
    # ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤— `ckpt_path` ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—ğŸ¤—
    ckpt_path = None
    SAVE_DIR = "./TB_RECODE"
    MODEL_NAME = "SNN"
    VERSION = "V1"
    tb_logger = TensorBoardLogger(save_dir=SAVE_DIR,
                                  name=MODEL_NAME,
                                  version=VERSION,  # TODO: å¦‚æœ ckpt_path æ˜¯ç©ºï¼Œåˆ°ä¸‹ä¸€ä¸ªç‰ˆæœ¬ï¼Œå¦‚æœä¸ä¸ºç©ºï¼Œè¯´æ˜åœ¨æ–­ç‚¹ç»­è®­
                                  log_graph=False)
    tb_logger.log_hyperparams(config)  # è®°å½•è¶…å‚æ•°

    # ---------------------
    # Model
    # ---------------------
    network_config = config.network_config
    assert network_config is not None
    net_module = fetch_net_module(network_config=network_config)

    # ---------------------
    # Callbacks and Misc
    # ---------------------
    callbacks = list()
    if config.training.lr_scheduler.use:
        callbacks.append(LearningRateMonitor(
            logging_interval="step"  # 'epoch' or 'step'
        ))

    # ---------------------
    # Training
    # ---------------------
    val_check_interval = config.validation.val_check_interval
    check_val_every_n_epoch = config.validation.check_val_every_n_epoch  # 1
    assert val_check_interval is None or check_val_every_n_epoch is None  # å…¶ä¸­ä¹‹ä¸€éœ€è¦æ˜¯ None
    # æŒ‡å®šå„ç§é…ç½®
    trainer = pl.Trainer(
        accelerator='gpu',
        callbacks=callbacks,
        enable_checkpointing=True,
        val_check_interval=val_check_interval,  # step åˆ°äº† 10000 çš„æ—¶å€™ä¼šæ‰§è¡Œ validation_stepï¼›è®­ç»ƒçš„æ—¶å€™
        check_val_every_n_epoch=check_val_every_n_epoch,  # å› ä¸ºval_check_interval ä¸ä¸º Noneï¼Œepoch æ²¡æœ‰ç”¨ï¼Œå› æ­¤è¿™ä¸ªéœ€è¦æ˜¯ None
        default_root_dir=None,
        devices=gpus,
        # gradient_clip_val=config.training.gradient_clip_val,  # 1.0
        gradient_clip_algorithm='value',
        # limit_train_batches=config.training.limit_train_batches,  # 1
        # limit_val_batches=config.validation.limit_val_batches,  # 1
        logger=tb_logger,
        # log_every_n_steps=config.logging.train.log_every_n_steps,  # self.logger.log_dict() æ¯è¿‡ log_every_n_steps æ¬¡è®°å½•ä¸€æ¬¡
        plugins=None,
        precision=16,  # ç›´æ¥åŠç²¾åº¦è®­ç»ƒ
        max_epochs=100,
        max_steps=30000,
        strategy=strategy,  # DDPStrategy  # pytorch_lightning 2.0.6 ä¸çŸ¥æ€çš„æŠ¥é”™, æ‰€ä»¥æˆ‘é€€å›åˆ°äº† 1.8.6
        sync_batchnorm=False if strategy is None else True,
    )

    trainer.fit(model=net_module, datamodule=data_module, ckpt_path=ckpt_path)


if __name__ == "__main__":
    main()
